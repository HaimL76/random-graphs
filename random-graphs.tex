\documentclass{article}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{graphicx} % Required for inserting images
\usepackage{xfrac}

\title{random-graphs}
\author{haiml76 }
\date{September 2025}

\begin{document}

\maketitle

\section{}
\subsection{}
Let $G_0\in{A}$ be a specific graph in the property $A$. The probability \[\mathbb{P}[G\sim{G(n,p)}=G_0],\]
where $G$ is a random graph in the model $G(n,p)$, is the probability for $G$ to have all the edges of $G_0$ and only them, that is,
\[\mathbb{P}[G\sim{G(n,p)}=G_0]=p^m(1-p)^{N-m},\]
where $m=e(G_0)$ is the number of edges in $G_0$ and $N=\binom{n}{2}$ is the number of possible edges between $n$ vertices.

The probability for a random $G$ to be in the property $A$ is the probability of the union of events $\{G\sim{G(n,p)}=H\}_{H\in{A}}$, but these events are distinct, hence \[\mathbb{P}[G\sim{G(n,p)}\in{A}]=\mathbb{P}[\bigcup_{H\in{A}}\{G\sim{G(n,p)}=H\}]=\sum_{H\in{A}}\mathbb{P}[G\sim{G(n,p)}=H]=\]\[=\sum_{H\in{A}}p^{m_H}(1-p)^{N-m_H},\]
where $m_H=e(H)$ is the number of edges in $H$ and $N=\binom{n}{2}$ as before. But this is a polynomial in $p$, hence a continuous function of $p$. 

In addition,
\[G(V,\phi)\notin{A}\Rightarrow\mathbb{P}[G\sim{G(n,0)}\in{A}]=0\]\[K_n\in{A}\Rightarrow\mathbb{P}[G\sim{G(n,1)}\in{A}]=1\]

Thus, by the intermediate value theorem, for any $0\leq{a}\leq{1}$ there exists $0\leq{p_a}\leq{1}$, s.t. $\mathbb{P}[G\sim{G(n,p_a)}\in{A}]=a$. We take $a=\frac{1}{2}$, so $p^\ast=p_\frac{1}{2}$.
\subsection{}
For every $0\leq{k}\leq{1}$ and $k\geq{0}$, $1-kp\leq(1-p)^k$, by simple induction,

For $k=0$, $1-0\cdot{p}=1\leq{(1-p)^0}=1$

For $k+1$, $(1-p)^{k+1}=(1-p)^k(1-p)\geq(1-kp)(1-p)=1-kp-p+kp^2=1-(k+1)p+kp^2\geq{1-(k+1)p}$.

Let $0\leq{q}\leq{1}$ be a probability s.t. $1-q=(1-p)^k$, by staged exposure it says that the two models $G(n,q)$ and $\bigcup_{i=1}^kG(n,p)$ are identical (have the same distribution), hence,
\[\mathbb{P}[G(n,q)\in{A}]=\mathbb{P}[\bigcup_{i=1}^kG(n,p)\in{A}]\Rightarrow\mathbb{P}[G(n,q)\notin{A}]=\mathbb{P}[\bigcap_{i=1}^kG(n,p)\notin{A}]=\]\[=\prod_{i=1}^k\mathbb{P}[G(n,p)\notin{A}]=(1-\mathbb{P}[G(n,p)\in{A}])^k=(1-q)^k,\]
because all distinct copies of $G(n,p)$ are independent.

But $1-kp\leq(1-p)^k=1-q\Rightarrow{kp\geq{q}}\Rightarrow\mathbb{P}[G(n,kp)\in{A}]\geq\mathbb{P}[G(n,q)\in{A}]$, because $A$ is a monotone \textbf{increasing} property. But then $\mathbb{P}[G(n,kp)\notin{A}]\leq\mathbb{P}[G(n,q)\notin{A}]=(\mathbb{P}[G(n,p)\notin{A}])^k$.
\subsection{}
$\omega\rightarrow\infty$, s.t. $\omega=o(n)$.

$\Rightarrow\forall{n>0}$, denote $\omega=\omega(n)$, there exists some $k$, s.t. 
\[k\leq\omega<k+1\Rightarrow{k}p^\ast\leq\omega{p^\ast}<(k+1)p^\ast\Rightarrow\]\[\Rightarrow\mathbb{P}[G(n,kp^\ast)\in{A}]\leq\mathbb{P}[G(n,\omega{p^\ast})\in{A}]\leq\mathbb{P}[G(n,(k+1)p^\ast)\in{A}]\Rightarrow\]\[\Rightarrow\mathbb{P}[G(n,(k+1)p^\ast)\notin{A}]\leq\mathbb{P}[G(n,\omega{p^\ast})\notin{A}]\leq\mathbb{P}[G(n,kp^\ast)\notin{A}],\] because $A$ is a monotone \textbf{increasing} property.

But, 
\[\mathbb{P}[G(n,kp^\ast)\notin{A}]\leq(\mathbb{P}[G(n,p^\ast)\notin{A}])^k=(1-\mathbb{P}[G(n,p^\ast)\in{A}])^k=\frac{1}{2^k}.\]
Thus, 
\[0\leq\lim_{k\rightarrow\infty}\mathbb{P}[G(n,(k+1)p^\ast)\notin{A}]\leq\lim_{k\rightarrow\infty}\mathbb{P}[G(n,kp^\ast)\notin{A}]\leq\lim_{k\rightarrow\infty}\frac{1}{2^k}=0\Rightarrow\]\[\Rightarrow\lim_{k\rightarrow\infty}\mathbb{P}[G(n,(k+1)p^\ast)\in{A}]\geq\lim_{k\rightarrow\infty}\mathbb{P}[G(n,kp^\ast)\in{A}]=1.\]
But then, by the Sandwich theorem, 
\[\lim_{n\rightarrow\infty}\mathbb{P}[G(n,\omega(n)p^\ast)\in{A}].\]

Also, $k\leq\omega<k+1\Rightarrow\frac{1}{k+1}<\frac{1}{\omega}\leq\frac{1}{k}\Rightarrow\frac{p^\ast}{k+1}<\frac{p^\ast}{\omega}\leq\frac{p^\ast}{k}$.
But,
$\frac{p^\ast}{k+1}\geq{0}$, and $\frac{p^\ast}{k}\leq\frac{1}{k}\Rightarrow\lim_{k\rightarrow\infty}\frac{1}{k}=0$, then by the Sandwich theorem, also $\lim_{n\rightarrow\infty}\frac{p^\ast}{\omega(n)}=0$, but then,
\[\lim_{n\rightarrow\infty}\mathbb{P}[G(n,\frac{p^\ast}{\omega(n)})\in{A}]=\lim_{p\rightarrow{0}}\mathbb{P}[G(n,p)\in{A}]=0,\]
because $\mathbb{P}[G(n,p)\in{A}]$ is continuous in $[0,1]$, and $\mathbb{P}[G(n,0)\in{A}]=0$.
\section{}
A graph process is identical to the model $G(n,m)$, as proved in class, so to prove the given claim, we shall use the $G(n,p)$ model, and then translate everything to $G(n,m)$. The proofs for the claims we are using shall be given \textbf{briefly}, because they can be found in textbooks class notes.

First, we observe that having a triangle is a monotone \textbf{increasing} property, because once we have a triangle in $G$, adding more edges will keep the existing triangle, while removing edges may result in omitting edges that belong to the triangle. Also, being connected is monotone \textbf{increasing} property, because once $G$ is connected, adding more edges will keep it connected, while removing edges may result in omitting edges that connect vertices that are otherwise isolated.

For this reason, we can look for thresholds both to having a triangle and being connected, and calculate the probability to have a triangle before being connected.

Then, we calculate the threshold for having a triangle in a random $G(n,p)$ graph. Let $X$ be the number of triangles in $G\sim{G(n,p)}$, for which we calculate the expectation, $\mathbb{E}[X]=\binom{n}{3}p^3=\frac{n(n-1)n-2)}{3!}p^3$, that is, all the choices of $3$ vertices in $G$, together with the probability for each choice of $3$ vertices to form a triangle. For any $p=p(n)$ s.t. $\mathbb{E}[X]\approx\frac{(np)^3}{6}\rightarrow{0}$ as $n\rightarrow\infty$, we get by Markov inequality that $\mathbb{P}[X\geq{1}]\leq\frac{\mathbb{E}[X]}{1}\rightarrow{0}$. But for $p(n)=\frac{c\log{n}}{n}$, where $c>0$ is some small constant, we get $\mathbb{E}[X]=\frac{n^3}{6}\cdot\frac{(c\log{n})^3}{n^3}=\frac{c^3(\log{n})^3}{6}\rightarrow\infty$ as $n\rightarrow\infty$, so the expectation $\mathbb{E}[X]$ itself is not enough. We calculate the expectation $\mathbb{E}[X^2]$. But 
\[X^2=\sum_{i,j=1}^{N_3}[T_i,T_j\in{G}],\]
where $N_3=\binom{n}{3}$ is the number of possible triangles (choices of $3$ vertices) in $G$.

This splits into several distinct options for joint edges. For $0$ joint edges, the expectation (we denote $\mathbb{E}[X_0^2]$) to find independently $T_i$ then $T_j$ is the number of choices of $3$ out of $n$ vertices, multiplied by the probability for these chosen vertices to share a triangle, then the number of choices of $3$ out of $n-3$ vertices, multiplied by the same probability as before, so in this case $\mathbb{E}[X_0^2]=\binom{n}{3}\binom{n-3}{3}p^6\approx\frac{(np)^6}{36}$, but checking the other options, we get that the expectation for $1,2,3$ joint edges is $\mathbb{E}[X_1^2]\approx{n^5p^6}$, $\mathbb{E}[X_2^2]\approx{n^4p^5}$ and $\mathbb{E}[X_3^2]\approx{n^3p^3}$, respectively, hence the total expectation, which is a sum, because these different options are distinct events, remains $\mathbb{E}[X^2]\approx(np)^6+o((np)^6)$.

We use a second-moment method, saying that \[\mathbb{P}[X>0]\geq\frac{(\mathbb{E}[X])^2}{\mathbb{P}[X^2]}\approx\frac{((np)^3)^2}{(np)^6+o((np)^6)}=\]\[=\frac{(np)^6}{(np)^6+o((np)^6)}\approx{1-o((np)^6)}\rightarrow{1},\]
as $n\rightarrow\infty$.
Thus, $\frac{c\log{n}}{n}$ is a threshold for having a triangle in $G$, for any small constant $c$.

Now, we calculate the threshold for $G$ to be connected. But this reduces to the threshold of not having isolated vertices in $G$ (takes some work to show this).

Let $X$ be the number of isolated vertices in $G$. 

A specific vertex $v_0$ is isolated if there is no edge between $v_0$ and the other $n-1$ vertices, the probability for this is $(1-p)^{n-1}$, and it is the same for every vertex in $G$, hence $\mathbb{E}[X]=n(1-p)^{n-1}$.

For the same $p(n)=\frac{c\log{n}}{n}$, we get 
\[n(1-p)^{n-1}=n(1-\frac{c\log{n}}{n})^{n-1}\rightarrow{n}e^{-c\log{n}},\]
as $n\rightarrow\infty$.
But $ne^{-c\log{n}}=e^{\log{n}(1-c)}=n^{1-c}$, and for any $0<{c}<1$, $n^{1-c}\rightarrow\infty$, as $n\rightarrow\infty$.

$\Rightarrow\mathbb{E}[X]\rightarrow\infty$, for $p=\frac{c\log{n}}{n}$, as $n\rightarrow\infty$, but the expectation itself is not enough. Again, we calculate $\mathbb{E}[X^2]$, but that means the expectation for $X_j$ isolated vertices when we have $X_i$ isolated vertices, but this breaks into two distinct events, when we count the same vertex $v_i$ twice, hence $\mathbb{E}[X_i^2]=\mathbb{E}[X]=n(1-p)^{n-1}$, and when the two vertices $v_i,v_j$ are distinct, hence $\mathbb{E}[X_iX_j]=n(1-p)^{n-1}(n-1)(1-p)^{n-2}$, because after we have $v_i$ isolated, then for $v_j$ to be isolated we have only $n-2$ other vertices that must not be adjacent to $v_j$ (because $v_i$ is already given as isolated).

\[\Rightarrow\mathbb{E}[X^2]=n(1-p)^{n-1}+n(n-1)(1-p)^{2n-3}\Rightarrow\]\[\Rightarrow{Var(X)=\mathbb{E}[X^2]-\mathbb{E}[X]^2}=\]\[=n(1-p)^{n-1}+n(n-1)(1-p)^{2n-3}-[(n(1-p)^{n-1}]^2=\]\[=n(1-p)^{n-1}+n(1-p)^{2n-3}(np-1).\]
Thus, by Chebyshev inequality,

$\mathbb{P}[X=0]\leq\mathbb{P}[|X-\mathbb{E}[X]|\geq\mathbb{E}[X]]\leq\frac{Var(X)}{\mathbb{E}[X]^2}=\frac{1}{\mathbb{E}[X]}+\frac{np-1}{n(1-p)}$. 

But $\mathbb{E}[X]\rightarrow\infty\Rightarrow\frac{1}{\mathbb{E}[X]}\rightarrow{0}\Rightarrow\mathbb{P}[X=0]\leq\frac{np-1}{n(1-p)}\leq\frac{p}{1-p}$.

But $p=\frac{c\log{n}}{n}\rightarrow{0}$ as $n\rightarrow\infty$, for any constant $c$, hence, 
\[\lim_{n\rightarrow\infty}\mathbb{P}[X=0]=0\Rightarrow\lim_{n\rightarrow\infty}\mathbb{P}[X>0]=1.\]
Thus, for any small constant $c>0$, a.a.s there exists at least one isolated vertex, that is, $G$ is not connected.
Thus, in conclusion, the probability $p=p(n)=\frac{c\log{n}}{n}$, where $c>0$ is any small constant, $p(n)$ is a threshold for $G$ to have at least one triangle, and a threshold for $G$ to have at least one isolated vertex (so not connected), and because $G(n,m)$ assumes the thresholds from $G(n,p)$, through the relation $m\approx\binom{n}{2}p$, we get that $m\approx\binom{n}{2}p(n)=\binom{n}{2}\frac{c\log{n}}{n}=\frac{cn(n-1)\log{n}}{2n}=\frac{c}{2}(n-1)\log{n}\rightarrow\infty$, as $n\rightarrow\infty$.
Thus, we get that a.a.s the graph process has at least one triangle before it is connected.
\section{}
Graph planarity is a monotone \textbf{decreasing} property, because if $G$ is planar, then removing edges will keep it planar, while adding edges may turn it to non-planar.

If a graph $G$ is planar, then $G$ is $5$-degenerate. We say that $G$ is $k$-degenerate, if for every permutation $\sigma$ of the graph $n$ vertices, each vertex $\sigma(i)$ can be adjacent with no more than $k$ distinct vertices $\sigma(j)$, s.t. $\sigma(j)>\sigma(i)$. But then $G$ has no more than $kn$ in our case $5n$ possible edges. Hence, the number of choices of $m$ edges out of $5n$ possible edges if $\binom{5n}{m}$, and there are $n!$ permutations on $n$, that is, we have $n!\binom{5n}{m}$ choices for $G$. Thus, the probability to have $G\sim{G(n,m)}$ a $5$-degenerate graph is $p^\ast=\frac{n!\binom{5n}{m}}{\binom{N}{m}}$, where $N=\binom{n}{2}$.

But $p^\ast=\frac{n!\binom{5n}{m}}{\binom{N}{m}}=n!\frac{5n(5n-1)(5n-2)\cdots(5n-m+1)}{N(N-1)(N-2)\cdots(N-m+1)}$.

We claim that $N\geq5n$, starting from some $n_0$, and we check,

Assume that $\binom{n_0}{2}=\frac{n_0(n_0-1)}{2}\geq5n_0\Rightarrow{n_0-1\geq10\Rightarrow{n_0\geq11}}$.

Hence, for every $n\geq11$,

$\frac{5n-k}{N-k}\leq\frac{5n-(k+1)}{N-(k+1)}$, for every $0\leq{k}\leq{m-1}$ 

(because $5nN-kN-N-5nk+k^2+k\leq5nN-5nk-5n-kN+k^2+k$, for $N\geq5n$).

Thus $p^\ast=n!\frac{5n(5n-1)(5n-2)\cdots(5n-m+1)}{N(N-1)(N-2)\cdots(N-m+1)}\leq\frac{n!(5n)^m}{N^m}$.

Hence, if $m=(1+\epsilon)n$, then,

\[
p^\ast\leq{n!(\frac{5n}{N})^m}=n!(\frac{5n}{N})^{(1+\epsilon)n}=n!(2\frac{5n}{n(n-1)})^{(1+\epsilon)n}=n!(\frac{10}{n-1})^{(1+\epsilon)n}.
\]
But by Stirling, $n!\approx\sqrt{2\pi{n}}(\frac{n}{e})^n$,

\[p^\ast\leq{n!(\frac{10}{n-1})^{(1+\epsilon)n}}\approx\sqrt{2\pi{n}}(\frac{n}{e})^n10^{(1+\epsilon)n}(n-1)^{(1+\epsilon)n}\leq\]\[\leq\sqrt{2\pi}n^{n+\frac{1}{2}}e^{-n}10^{(1+\epsilon)n}n^{-(1+\epsilon)n}\leq{n^{\frac{1}{2}-\epsilon{n}}e^{-n}10^{(1+\epsilon)n}}\rightarrow{0},
\]
as $n\rightarrow\infty$.

Thus, for $m=(1+\epsilon)n$, the limit probability for $G$ to be $5$-degenerate is {0}, hence the limit probability for $G$ to be non-planar is $1$, and since planarity is monotone \textbf{decreasing}, we get that it is a threshold value, that is, for every $m\geq(1+\epsilon)n$, a.a.s. $G$ is non-planar.
\section{}
\subsection{}
For $n$ vertices, we have $n!$ permutations, but Hamilton cycles are indifferent to the starting point (since they are \textbf{cycles}), hence we divide the number of permutations by the number of starting points for each cycle, that is $n$. Hamilton cycles are also indifferent to direction, hence we divide this count by $2$ and we get $\frac{(n-1)!}{2}$. For each permutation $\sigma$ of the $n$ vertices, each edge in $\mu(F)$ comes from an edge between two vertices in the consecutive layers of $W$ ordered by $\sigma$, but that is $d$ choices of vertices from layers $\sigma(1),\sigma(2)$, then $d-1$ choices of the remaining vertices from layer $\sigma(k)$ together with $d$ choices of vertices from layer $\sigma(k+1)$, for every $2\leq{k}\leq{n-1}$, then $d-1$ choices of the remaining vertices from layers $\sigma(n),\sigma(1)$, but because all the choices are independent, we have $d^2(d(d-1))^{n-2}(d-1)^2=(d(d-1))^n$ choices.

We calculate the number of choices for constructing $F$ s.t. $\mu(F)$ is Hamiltonian, and the total number of choices for constructing $F$.

Denote by $m$ the number of available vertices, for both constructions, we need to choose $\frac{m}{2}$ pairs out of $m$ vertices, that is $\frac{m!}{2^{\sfrac{m}{2}}}$, but the order between the pairs is insignificant, hence we get $\frac{m!}{2^{\sfrac{m}{2}}(\sfrac{m}{2})!}$

For the general construction we set $m=nd$, and for the Hamiltonian construction we set $m=n(d-2)$, because we already chose $2$ vertices of each of the $n$ layers. Thus, the probability to choose uniformly a Hamiltonian construction is $\frac{\frac{(n(d-2))!}{2^{\sfrac{n(d-2)}{2}}(\sfrac{n(d-2)}{2})!}}{\frac{(nd)!}{2^{\sfrac{nd}{2}}(\sfrac{nd}{2})!}}$, hence, multiplying these two results, we get \[
\mathbb{E}[H]=\frac{(n-1)!}{2}(d(d-1))^n\frac{\frac{(n(d-2))!}{2^{\sfrac{n(d-2)}{2}}(\sfrac{n(d-2)}{2})!}}{\frac{(nd)!}{2^{\sfrac{nd}{2}}(\sfrac{nd}{2})!}}
.\]
Simplifying this expression, we get
\[
\mathbb{E}[H]=\frac{(n-1)!}{2}(d(d-1))^n\frac{(n(d-2))!2^{\sfrac{nd}{2}}(\sfrac{nd}{2})!}{(nd)!2^{\sfrac{n(d-2)}{2}}(\sfrac{n(d-2)}{2})!}
.\]
\subsection*{4.b}
By Stirling, $n!\approx\sqrt{2\pi{n}}(\frac{n}{e})^n$, hence 
\[\mathbb{E}[H]\approx\frac{\sqrt{2\pi n}\,\left(\tfrac{n}{e}\right)^n}{2n}
\cdot(d(d-1))^n\cdot\]\[
\quad\cdot\frac{\sqrt{2\pi{n(d-2)}}\left(\tfrac{n(d-2)}{e}\right)^{n(d-2)}\cdot{2^{nd/2}}\cdot\sqrt{\pi nd}\cdot\left(\tfrac{nd/2}{e}\right)^{nd/2}}
         {\sqrt{\pi nd/2}\left(\tfrac{n(d-2)/2}{e}\right)^{n(d-2)/2}\cdot 2^{nd/2}\cdot2^{-n}\cdot\sqrt{2\pi nd}\cdot\left(\tfrac{nd}{e}\right)^{nd}}
=\]\[=\frac{\sqrt{2\pi{n}}\,\left(\tfrac{n}{e}\right)^n}{2n}\cdot(d(d-1))^n\cdot\]
\[\quad\cdot2^n\cdot e^{nd+\tfrac{n(d-2)}{2}-\tfrac{nd}{2}-n(d-2)}\cdot{n}^{-nd- \tfrac{n(d-2)}{2}+\tfrac{nd}{2}+n(d-2)}
\quad\cdot(d-2)^{\tfrac{n(d-2)}{2}} \cdot{d}^{-nd/2}\cdot{2^{-n}}
=\]\[=\frac{\sqrt{2\pi n}\,\left(\tfrac{n}{e}\right)^n}{2n}\cdot(d(d-1))^n\cdot{e^n} \cdot{n^{-n}}
    \cdot{d}^{-\tfrac{nd}{2}}\cdot(d-2)^{\,n(-1+\tfrac{d}{2})}
=\]\[=\sqrt{\frac{\pi}{2n}}\,(d-1)^n\,d^{\,n(1-\tfrac{d}{2})}\cdot(d-2)^{\,-n(1-\tfrac{d}{2})}
\approx\sqrt{\frac{\pi}{2n}}\,\Big[(d-1)\,\left(\tfrac{d-2}{d}\right)^{\tfrac{d-2}{2}}\Big]^n.\]
\subsection*{4.c}

\subsection*{4.d}
The following calculation shows the number of choices to construct all $2$-factors, along with constructions that are not $2$-factors, and hence it is an \textbf{upper bound} for the desired number of choices.

We need to choose $n$ pairs out of $2n$ vertices, two of each layer, that is $\frac{(2n)!}{2^n}$, but the order of choosing them is insignificant, hence the number of choices is $\frac{(2n)!}{n!2^n}$.
Each $2$-factor is constructed by choosing $2$ vertices of each layers, that is $\binom{d}{2}^n$. Thus, the total number of choices is at most $\binom{d}{2}^n\frac{(2n)!}{n!2^n}$.

We calculate the probability, using the number of choices for pairs that we found earlier, that is $\frac{m!}{\frac{m}{2}!2^{\frac{m}{2}}}$, where $m$ is the total number of vertices to choose from.

For the total number of configurations we set $m=nd$, and for the number of configurations that can construct a $2$-factor we set $m=n(d-2)$, thus the probability is $
\frac{\frac{(n(d-2))!}{(\frac{n(d-2)}{2})!2^{\frac{n(d-2)}{2}}}}{\frac{(nd)!}{(\frac{nd}{2})!2^{\frac{nd}{2}}}}=\frac{(n(d-2))!(\sfrac{nd}{2})!2^{\sfrac{nd}{2}}}{(nd)!(\sfrac{n(d-2)}{2})!2^{\sfrac{n(d-2)}{2}}}$.

Thus, the expectation $\mathbb{E}[T]\leq\binom{d}{2}^n\frac{(2n)!}{n!2^n}\frac{(n(d-2))!(\sfrac{nd}{2})!2^{\sfrac{nd}{2}}}{(nd)!(\sfrac{n(d-2)}{2})!2^{\sfrac{n(d-2)}{2}}}$.

\subsection*{4.e}
Let $T$ be as above (the number of $2$-factors in $\mu(F)$), and denote $T_d$ the number of $2$-factors in $G_d=(V,E)=([n],E)$, where $G_d$ is $d$-regular. Denote by $S$ the number of simple graphs of the form $\mu(F)$, then the expectation $\mathbb{E}[T_d]=\mathbb{E}[T|S]\leq\frac{\mathbb{E}[T]}{\mathbb{P}[S]}=e^{\frac{d^2-1}{4}}O(\mathbb{E}[T])$.

But by Markov inequality,

$\mathbb{P}[T_d\geq{n\mathbb{E}[T]}]\leq\frac{\mathbb{E}[T_d]}{n\mathbb{E}[T]}=\frac{e^{\frac{d^2-1}{4}}O(\mathbb{E}[T])}{n\mathbb{E}[T]}$.

But $f=O(\mathbb{E}[T])$ means that asymptotically, $|f|\leq{c\mathbb{E}[T]}$, for some constant $c$, hence, asymptotically $\frac{e^{\frac{d^2-1}{4}}O(\mathbb{E}[T])}{n\mathbb{E}[T]}\leq\frac{c\mathbb{E}[T]}{n\mathbb{E}[T]}=\frac{c}{n}\rightarrow{0}$, as $n\rightarrow\infty$.

\[
\Rightarrow\lim_{n\rightarrow\infty}\mathbb{P}[T_d<n\mathbb{E}[T]]=1-\lim_{n\rightarrow\infty}\mathbb{P}[T_d\geq{n\mathbb{E}[T]}]=1
\]

Let $T_G$ be the random variable counting the 2-factors of a n-vertices d-regular graph $G$. Notice that every multi-graph without loops and multi-edges i.e. simple made by the configuration model is a d-regular graph. Therefore, $T_G = T | \, \mu(F) \, \text{is simple}$.\\
\[
\Longrightarrow{} \mathbb{E}[T_G]=\mathbb{E}[T | \, \mu(F) \, \text{is simple}]\underset{\text{total expectation}}{\leq} \frac{\mathbb{E}[T]}{\mathbb{P}(\mu(F) \, \text{is simple})}\underset{\substack{\text{shown in } \\ \text{the lecture}}}{= }e^{\tfrac{d^2-1}{4}} O(\mathbb{E}[T])
\]
Now we can use Markov's inequality:\\
\[
\mathbb{P}(T_G>n\mathbb{E}[T])\leq\frac{\mathbb{E}[T_G]}{n\mathbb{E}[T]}=\frac{e^{\tfrac{d^2-1}{4}} O(\mathbb{E}[T])}{n\mathbb{E}[T]} \leq \frac{c}{n}\xrightarrow{n \to \infty}0
\]
Therefore, a.a.s $T_G<n\mathbb{E}[T]$.\\
\text{                                        }\hfill $\square$

\subsection*{4.f}
We use $T_d$ from earlier, and we denote $H_d$ the number of Hamilton cycles in a random $d$-regular graph. 

We already saw that $\mathbb{E}[H_d]\geq\frac{\mathbb{E}[H]}{n}$ and that $\mathbb{E}[T_d]\leq{n\mathbb{E}[T]}$.

\[\Rightarrow\frac{\mathbb{E}[H_d]}{\mathbb{E}[T_d]}\geq\frac{\mathbb{E}[H]}{2\mathbb{E}[T]}
\geq
\frac{\mathbb{E}[H]}{n^2 \mathbb{E}[T]} \\[2mm]
\geq
\frac{
\dfrac{(n-1)!}{2} (d(d-1))^n \cdot
\dfrac{(n(d-2))! \, 2^{nd/2} (nd/2)!}{(nd)! \, 2^{n(d-2)/2} (n(d-2)/2)!}
}{
n^2 \cdot \binom{d}{2}^n \cdot 
\dfrac{(2n)!}{n! 2^n} \cdot 
\dfrac{(n(d-2))! (\tfrac{nd}{2})! 2^{nd/2}}{(\tfrac{n(d-2)}{2})! 2^{n(d-2)/2} (nd)!}
}
=\]\[=
\frac{
\dfrac{1}{2} (n-1)! (d(d-1))^n
}{
n^2 \cdot \dfrac{(d(d-1))^n}{2^n} \cdot \dfrac{(2n)!}{n! 2^n}
}
=\frac{1}{2}\cdot2^{2n}\cdot n^{-2}\cdot\frac{n!(n-1)!}{(2n)!
}
\approx\frac{1}{2}\cdot2^{2n}\cdot n^{-3}\cdot\frac{2\pi n(\frac{n}{e})^{2n}}{\sqrt{2\pi 2n}(\frac{2n}{e})^{2n}
}
=\]\[=\frac{1}{2}\cdot2^{2n}\cdot2^{-2n}\cdot n^{-2.5}\sqrt{\pi
}
\geq\frac{1}{2} n^{-2.5}.\]

\subsection*{4.g}
We will use the construction from Tutte's paper. Let $G'$ be the new graph(for $f(a)=2\  \ \forall a \in V$).\\
We will use the perfect matching algorithm on $G'$. The matched edges define a spanning subgraph of $G'$.
Let $a \in V$, we check how do the vertices in $X(a)\cup Y(a)$ match.\\
All the $s(a)$ vertices in $Y(a)$ have to be paired to vertices in $X(a)$, since they are connected only to vertices of $X(a)$ according to the construction. Now we need to check how many vertices of $X(a)$ are not paired to vertices from $Y(a)$:\\
\[
d(a)-s(a)=d(a)-(d(a)-f(a))=f(a)=2
\]\\
Therefore, there are exactly two vertices in $X(a)$ who are not connected to vertices of $St(a)$ (they cannot be matched together according to the construction).\\
Therefore, there are two original edges of $G$ connecting $St(a)$ to two other stars. Since this is true for every single $a \in V$, all the edges together define a 2-factor on $G$.
\hfill $\square$

\subsection*{4.h}
phase 1: as in (g), we build $G'$, and use the perfect matching algorithm. We get a 2-factor on $G$.\\
phase 2: in (f) we proved that the ratio between the number of the Hamilton circles to the number of 2-factors in a d-regular graph is at least $\frac{1}{2n^{2.5}}$ a.a.s.. Therefore if we repeat phase 1 at least $n^{3.5}$ times we have:
\[
\mathbb{P}(\{\substack{\text{not getting Hamilton} \\ \text{circle even once}}\})\leq(1-\frac{1}{2n^{2.5}})^{n^{3.5}}=((1-\frac{1/2}{n^{2.5}})^{n^{2.5}})^n\approx e^{-\frac{1}{2}n}\xrightarrow{n \to \infty}0
\]\\
Therefore a.a.s. we get a Hamilton circle in $G$.\\
Since the matching algorithm is polynomial, and we use it $n^{3.5}$ times, the total algorithm is polynomial.
\hfill $\square$
\section{}
$G=G(V,E)$.
\subsection{}
We define the following first order formula,

$\varphi(x)=\exists{y,z}$ . $y\neq{x}\land{z\neq{x}}\land{y\neq{z}}\land{x\sim{y}}\land{y\sim{z}}\land{z\sim{x}}$

Or, under the assumption that $G$ is always a simple graph,

$\varphi(x)=\exists{y,z}$ . $x\sim{y}\land{y\sim{z}}\land{z\sim{x}}$

Denote $U:=\{u\in{V}\,|\,{G\vDash\varphi(u)}\}$, then for every $x$, either $x\in{U}$ or $x\notin{U}$.
\subsection{}
$G=(V,E)=([n],E)$, where $n=\infty$.

We define a first order language sentence, with $v,y,z,x_1,x_2,\dots,x_k$ distinct,
\[\theta_k(v,y,z)=\forall{x_1,x_2,\dots,x_k}\in{V}\,.\]\[.\,\neg(y\sim{x_1})\lor\neg(x_k\sim{z})\bigvee_{i=1}^{k-1}\neg(x_i\sim{x_{i+1}})\bigvee_{i=1}^{k-1}(x_i\sim{v}\land{v\sim{x_{i+1}}}).\]
This sentence says that any path of length $k$ between $y$ and $z$ must pass through $v$.

Suppose that there exists $\varphi(v)$ a first order language, defined as $\varphi(v)=$\textbf{v is a cut vertex}.

We define a theory $T$,
\begin{enumerate}
    \item $\exists{y,z}\,.\,\neg(y=v)\land\neg(z=v)\land\neg(y=z)\land{y\sim{v}}\land{z\sim{v}}\land\neg(y\sim{z})$.
    \item 
    $\bigwedge_{k=1}^\infty\theta_k(v,y,z)$.
    \item 
    $\neg\varphi(v)$.
\end{enumerate}

This theory is infinite, because $n=\infty$. But by the compactness theorem, $T$ is satisfiable is and only if every finite subset $S\subset{T}$ is satisfiable.
Then take some arbitrary integer $m>0$, and define $T_m\subset{T}$ with all the sentences, except that we take $\bigwedge_{k=1}^m\theta_k(v,y,z)$ instead of the infinite $\bigwedge$.
But then we simply build a graph $G_m=([n],E)$, with two distinct vertices that are adjacent to $v$ and there exists a path of length $m+1$ between $y$ and $z$, but there are no paths of length $\leq{m}$, hence $T_m$ is satisfiable but $T$ is not, in contradiction to the compactness theorem.
\subsection{}
The empty set can be defined by the following first order formula,

$\varphi_\phi(x)=x\neq{x}$, which is a contradiction, hence 

$U_\phi:=\{u\in{V}\,|\,{G\vDash\varphi_\phi(u)}\}=\phi$.

The set of all graph vertices can be defined by the following first order formula,

$\varphi_V(x)=x=x$, which is a tautology, hence $U_V:=\{u\in{V}\,|\,{G\vDash\varphi_V(u)}\}=V$.

Both formulas are either true or false for all $v\in{V}$, therefore the sets that satisfy them are called the trivial definable sets.

Comment: there are specific symbols in first order languages, to specify tautology and contradiction, but I prefer the above sentences.
\subsection{}
The required statement would be,
\noindent
\[\tilde\varphi=\forall{0\leq{p}\leq{1}},\epsilon>0\,.\,\exists{N}\,.\,\forall{n>N},\varphi\,.\,\]\[.\,\varphi=\varphi_\phi\lor\varphi=\varphi_V\lor\mathbb{P}[G(n,p)\vDash\varphi]<\epsilon\]

Another version of $\tilde\varphi$,
\noindent
\[\tilde\varphi=\forall{0\leq{p}\leq{1}},\epsilon>0\,.\,\exists{N}\,.\,\forall{n>N},\varphi\,.\,\]\[.\,\neg(\exists{v,u\in{V}}\,.\,\varphi\neq\varphi_\phi\land\varphi\neq\varphi_V\land\mathbb{P}[G(n,p)\vDash\varphi(u)]\geq\epsilon\land\mathbb{P}[G(n,p)\neg\vDash\varphi(v)]\geq\epsilon)\]
\subsection{}
This should be immediate from Fagin's 0-1 law, because it applies to every constant probability $0\leq{p}\leq{1}$, that is, for every constant $p$, and for every first order sentence $\varphi$,
\[
\lim_{n\rightarrow\infty}\mathbb{P}[G\sim{G(n,p)}\vDash\varphi]\in\{0,1\}\Rightarrow{U_\varphi\in\{\phi,V\}}.
\]

But I shall describe my idea of a proof.

For every $x\in{V}$, a sentence $\varphi(x)$ is a finite set of statements which can contain a finite subset of the graph vertices $x_1,x_2\dots,x_k\in{V}$, for some $k<\infty$, the quantifiers $\exists,\forall$, the relations $=,\sim$, and the negation unary logical operator $\neg$. These statements are related to each other by the logical operators $\land,\lor$.

The idea of the proof is by induction on the total number of elements in the sentence.

We start from one statement with one quantifier, one vertex and one relation.

$\varphi(x)=\exists{y\in{V}}\,.\,y={x}$ would be a tautology, so we take 

$\varphi(x)=\exists{y\in{V}}\,.\,y\neq{x}\land{y={x}}$ (we actually added more elements implicitly, but that does not matter).

Fix $n>0$, $\mathbb{P}[G\vDash\varphi(x)]$ is the probability that there exists an edge between $x$ and some other vertex in the graph, which is the completion of the event that $x$ does not have an edge with any other graph vertex, that is \[\mathbb{P}[G\sim{G(n,p)}\neg\vDash\varphi(x)]=\prod_{x\neq{y}}\mathbb{P}[x\nsim{y}]=\prod_{y\neq{x}}1-\mathbb{P}[x\sim{y}]=\]\[=\prod_{i=1}^{n-1}1-p=(1-p)^{n-1}=\frac{(1-p)^n}{1-p}\]
but \[\lim_{n\rightarrow\infty}\frac{(1-p)^n}{1-p}=0\]
hence for every $x\in{V}$, \[\lim_{n\rightarrow\infty}\mathbb{P}[G(n,p)\vDash\varphi(x)]=1-\lim_{n\rightarrow\infty}\mathbb{P}[G(n,p)\neg\vDash\varphi(x)]=1\Rightarrow{V_\varphi={V}}.\]
This assumes that $p$ is constant for every $n>0$, otherwise there might not be a convergence (to zero).

Same way, we can prove that if $\varphi(x)=\forall{y\neq{x}}\,.\,x\sim{y}$, then \[\lim_{n\rightarrow\infty}\mathbb{P}[G(n,p)\vDash\varphi(x)]=\lim_{n\rightarrow\infty}p^{n-1}=\lim_{n\rightarrow\infty}\frac{p^n}{p}=0\Rightarrow{V_\varphi=\phi}.\]
Thus we assume that for any sentence $\varphi$ of $m$ elements, either

\[\lim_{n\rightarrow\infty}\mathbb{P}[G\sim{G(n,p)\vDash\varphi}]=0\Rightarrow\lim_{n\rightarrow\infty}\mathbb{P}[G\sim{G(n,p)\vDash\neg\varphi}]=1\]

or,

\[\lim_{n\rightarrow\infty}\mathbb{P}[G\sim{G(n,p)\vDash\varphi}]=1\Rightarrow\lim_{n\rightarrow\infty}\mathbb{P}[G\sim{G(n,p)\vDash\neg\varphi}]=0\]

We need to prove for $m+1$ elements. Suppose that the additional element is another statement $\tilde\varphi$, the the relation between them is either $\varphi\land\tilde\varphi$ or $\varphi\lor\tilde\varphi$, but by the assumption, $\varphi,\tilde\varphi$ have limit probabilities, thus \[\lim_{n\rightarrow\infty
}\mathbb{P}[G\sim{G(n,p)}\vDash(\varphi\land\tilde\varphi)]=\lim_{n\rightarrow\infty
}\mathbb{P}[G\sim{G(n,p)}\vDash\varphi]\cdot\lim_{n\rightarrow\infty
}\mathbb{P}[G\sim{G(n,p)}\vDash\tilde\varphi]\]

and,

\[\lim_{n\rightarrow\infty
}\mathbb{P}[G\sim{G(n,p)}\vDash(\varphi\lor\tilde\varphi)]=\lim_{n\rightarrow\infty
}\mathbb{P}[G\sim{G(n,p)}\vDash\neg(\neg\varphi\land\neg\tilde\varphi)]=\]\[=1-\lim_{n\rightarrow\infty
}\mathbb{P}[G\sim{G(n,p)}\vDash\neg\varphi]\cdot\lim_{n\rightarrow\infty
}\mathbb{P}[G\sim{G(n,p)}\neg\vDash\tilde\varphi].\]

Suppose now that the additional element is another vertex $x_{m+1}$ in an existing expression of the form $\exists{x_1,x_2,\dots,x_m}$. But then we can express the new sentence as $\varphi\land\tilde\varphi=\varphi\land\exists{x_{m+1}}\,.\,\dots$
Both $V_\varphi$ and $V_{\tilde\varphi}$ are trivial, by the assumption, hence $V_{\varphi\land\tilde\varphi}=V_\varphi\cap{V_{\tilde\varphi}}$ is also trivial.
We can show further that every $m+1$ additional element is of the form $\varphi\land\tilde\varphi$ or $\varphi\lor\tilde\varphi$, and since $\varphi,\tilde\varphi$ have a limit probability of either $0$ or $1$, by the assumption, then $\varphi\land\tilde\varphi$ or $\varphi\lor\tilde\varphi$ have a limit probability of either $0$ or $1$.
\end{document}