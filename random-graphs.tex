\documentclass{article}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{graphicx} % Required for inserting images

\title{random-graphs}
\author{haiml76 }
\date{September 2025}

\begin{document}

\maketitle

\section{}
\subsection{}
Let $G_0\in{A}$ be a specific graph in the property $A$. The probability \[\mathbb{P}[G\sim{G(n,p)}=G_0],\]
where $G$ is a random graph in the model $G(n,p)$, is the probability for $G$ to have all the edges of $G_0$ and only them, that is,
\[\mathbb{P}[G\sim{G(n,p)}=G_0]=p^m(1-p)^{N-m},\]
where $m=e(G_0)$ is the number of edges in $G_0$ and $N=\binom{n}{2}$ is the number of possible edges between $n$ vertices.

The probability for a random $G$ to be in the property $A$ is the probability of the union of events $\{G\sim{G(n,p)}=H\}_{H\in{A}}$, but these events are distinct, hence \[\mathbb{P}[G\sim{G(n,p)}\in{A}]=\mathbb{P}[\bigcup_{H\in{A}}\{G\sim{G(n,p)}=H\}]=\sum_{H\in{A}}\mathbb{P}[G\sim{G(n,p)}=H]=\]\[=\sum_{H\in{A}}p^{m_H}(1-p)^{N-m_H},\]
where $m_H=e(H)$ is the number of edges in $H$ and $N=\binom{n}{2}$ as before. But this is a polynomial in $p$, hence a continuous function of $p$. 

In addition,
\[G(V,\phi)\notin{A}\Rightarrow\mathbb{P}[G\sim{G(n,0)}\in{A}]=0\]\[K_n\in{A}\Rightarrow\mathbb{P}[G\sim{G(n,1)}\in{A}]=1\]

Thus, by the intermediate value theorem, for any $0\leq{a}\leq{1}$ there exists $0\leq{p_a}\leq{1}$, s.t. $\mathbb{P}[G\sim{G(n,p_a)}\in{A}]=a$. We take $a=\frac{1}{2}$, so $p^\ast=p_\frac{1}{2}$.
\subsection{}
For every $0\leq{k}\leq{1}$ and $k\geq{0}$, $1-kp\leq(1-p)^k$, by simple induction,

For $k=0$, $1-0\cdot{p}=1\leq{(1-p)^0}=1$

For $k+1$, $(1-p)^{k+1}=(1-p)^k(1-p)\geq(1-kp)(1-p)=1-kp-p+kp^2=1-(k+1)p+kp^2\geq{1-(k+1)p}$.

Let $0\leq{q}\leq{1}$ be a probability s.t. $1-q=(1-p)^k$, by staged exposure it says that the two models $G(n,q)$ and $\bigcup_{i=1}^kG(n,p)$ are identical (have the same distribution), hence,
\[\mathbb{P}[G(n,q)\in{A}]=\mathbb{P}[\bigcup_{i=1}^kG(n,p)\in{A}]\Rightarrow\mathbb{P}[G(n,q)\notin{A}]=\mathbb{P}[\bigcap_{i=1}^kG(n,p)\notin{A}]=\]\[=\prod_{i=1}^k\mathbb{P}[G(n,p)\notin{A}]=(1-\mathbb{P}[G(n,p)\in{A}])^k=(1-q)^k,\]
because all distinct copies of $G(n,p)$ are independent.

But $1-kp\leq(1-p)^k=1-q\Rightarrow{kp\geq{q}}\Rightarrow\mathbb{P}[G(n,kp)\in{A}]\geq\mathbb{P}[G(n,q)\in{A}]$, because $A$ is a monotone \textbf{increasing} property. But then $\mathbb{P}[G(n,kp)\notin{A}]\leq\mathbb{P}[G(n,q)\notin{A}]=(\mathbb{P}[G(n,p)\notin{A}])^k$.
\subsection{}
$\omega\rightarrow\infty$, s.t. $\omega=o(n)$.

$\Rightarrow\forall{n>0}$, denote $\omega=\omega(n)$, there exists some $k$, s.t. 
\[k\leq\omega<k+1\Rightarrow{k}p^\ast\leq\omega{p^\ast}<(k+1)p^\ast\Rightarrow\]\[\Rightarrow\mathbb{P}[G(n,kp^\ast)\in{A}]\leq\mathbb{P}[G(n,\omega{p^\ast})\in{A}]\leq\mathbb{P}[G(n,(k+1)p^\ast)\in{A}]\Rightarrow\]\[\Rightarrow\mathbb{P}[G(n,(k+1)p^\ast)\notin{A}]\leq\mathbb{P}[G(n,\omega{p^\ast})\notin{A}]\leq\mathbb{P}[G(n,kp^\ast)\notin{A}],\] because $A$ is a monotone \textbf{increasing} property.

But, 
\[\mathbb{P}[G(n,kp^\ast)\notin{A}]\leq(\mathbb{P}[G(n,p^\ast)\notin{A}])^k=(1-\mathbb{P}[G(n,p^\ast)\in{A}])^k=\frac{1}{2^k}.\]
Thus, 
\[0\leq\lim_{k\rightarrow\infty}\mathbb{P}[G(n,(k+1)p^\ast)\notin{A}]\leq\lim_{k\rightarrow\infty}\mathbb{P}[G(n,kp^\ast)\notin{A}]\leq\lim_{k\rightarrow\infty}\frac{1}{2^k}=0\Rightarrow\]\[\Rightarrow\lim_{k\rightarrow\infty}\mathbb{P}[G(n,(k+1)p^\ast)\in{A}]\geq\lim_{k\rightarrow\infty}\mathbb{P}[G(n,kp^\ast)\in{A}]=1.\]
But then, by the Sandwich theorem, 
\[\lim_{n\rightarrow\infty}\mathbb{P}[G(n,\omega(n)p^\ast)\in{A}].\]

Also, $k\leq\omega<k+1\Rightarrow\frac{1}{k+1}<\frac{1}{\omega}\leq\frac{1}{k}\Rightarrow\frac{p^\ast}{k+1}<\frac{p^\ast}{\omega}\leq\frac{p^\ast}{k}$.
But,
$\frac{p^\ast}{k+1}\geq{0}$, and $\frac{p^\ast}{k}\leq\frac{1}{k}\Rightarrow\lim_{k\rightarrow\infty}\frac{1}{k}=0$, then by the Sandwich theorem, also $\lim_{n\rightarrow\infty}\frac{p^\ast}{\omega(n)}=0$, but then,
\[\lim_{n\rightarrow\infty}\mathbb{P}[G(n,\frac{p^\ast}{\omega(n)})\in{A}]=\lim_{p\rightarrow{0}}\mathbb{P}[G(n,p)\in{A}]=0,\]
because $\mathbb{P}[G(n,p)\in{A}]$ is continuous in $[0,1]$, and $\mathbb{P}[G(n,0)\in{A}]=0$.
\section{}
A graph process is identical to the model $G(n,m)$, as proved in class, so to prove the given claim, we shall use the $G(n,p)$ model, and then translate everything to $G(n,m)$. The proofs for the claims we are using shall be given \textbf{briefly}, because they can be found in textbooks class notes.

First, we observe that there is threshold for having a triangle in a random $G(n,p)$ graph. Let $X$ be the number of triangles in $G\sim{G(n,p)}$, for which we calculate the expectation, $\mathbb{E}[X]=\binom{n}{3}p^3=\frac{n(n-1)n-2)}{3!}p^3$, that is, all the choices of $3$ vertices in $G$, together with the probability for each choice of $3$ vertices to form a triangle. For any $p=p(n)$ s.t. $\mathbb{E}[X]\approx\frac{(np)^3}{6}\rightarrow{0}$ as $n\rightarrow\infty$, we get by Markov inequality that $\mathbb{P}[X\geq{1}]\leq\frac{\mathbb{E}[X]}{1}\rightarrow{0}$. But for $p(n)=\frac{c\log{n}}{n}$, where $c>0$ is some small constant, we get $\mathbb{E}[X]=\frac{n^3}{6}\cdot\frac{(c\log{n})^3}{n^3}=\frac{c^3(\log{n})^3}{6}\rightarrow\infty$ as $n\rightarrow\infty$, so the expectation $\mathbb{E}[X]$ itself is not enough. We calculate the expectation $\mathbb{E}[X^2]$. But 
\[X^2=\sum_{i,j=1}^{N_3}[T_i,T_j\in{G}],\]
where $N_3=\binom{n}{3}$ is the number of possible triangles (choices of $3$ vertices) in $G$.

This splits into several distinct options for joint edges. For $0$ joint edges, the expectation (we denote $\mathbb{E}[X_0^2]$) to find independently $T_i$ then $T_j$ is the number of choices of $3$ out of $n$ vertices, multiplied by the probability for these chosen vertices to share a triangle, then the number of choices of $3$ out of $n-3$ vertices, multiplied by the same probability as before, so in this case $\mathbb{E}[X_0^2]=\binom{n}{3}\binom{n-3}{3}p^6\approx\frac{(np)^6}{36}$, but checking the other options, we get that the expectation for $1,2,3$ joint edges is $\mathbb{E}[X_1^2]\approx{n^5p^6}$, $\mathbb{E}[X_2^2]\approx{n^4p^5}$ and $\mathbb{E}[X_3^2]\approx{n^3p^3}$, respectively, hence the total expectation, which is a sum, because these different options are distinct events, remains $\mathbb{E}[X^2]\approx(np)^6+o((np)^6)$.

We use a second-moment method, saying that \[\mathbb{P}[X>0]\geq\frac{(\mathbb{E}[X])^2}{\mathbb{P}[X^2]}\approx\frac{((np)^3)^2}{(np)^6+o((np)^6)}=\]\[=\frac{(np)^6}{(np)^6+o((np)^6)}\approx{1-o((np)^6)}\rightarrow{1},\]
as $n\rightarrow\infty$.
Thus, $\frac{c\log{n}}{n}$ is a threshold for having a triangle in $G$, for any small constant $c$.

Now, we calculate the threshold for $G$ to be connected. But this reduces to the threshold of not having isolated vertices in $G$ (takes some work to show this).

Let $X$ be the number of isolated vertices in $G$. 

A specific vertex $v_0$ is isolated if there is no edge between $v_0$ and the other $n-1$ vertices, the probability for this is $(1-p)^{n-1}$, and it is the same for every vertex in $G$, hence $\mathbb{E}[X]=n(1-p)^{n-1}$.

For the same $p(n)=\frac{c\log{n}}{n}$, we get 
\[n(1-p)^{n-1}=n(1-\frac{c\log{n}}{n})^{n-1}\rightarrow{n}e^{-c\log{n}},\]
as $n\rightarrow\infty$.
But $ne^{-c\log{n}}=e^{\log{n}(1-c)}=n^{1-c}$, and for any $0<{c}<1$, $n^{1-c}\rightarrow\infty$, as $n\rightarrow\infty$.

$\Rightarrow\mathbb{E}[X]\rightarrow\infty$, for $p=\frac{c\log{n}}{n}$, as $n\rightarrow\infty$, but the expectation itself is not enough. Again, we calculate $\mathbb{E}[X^2]$, but that means the expectation for $X_j$ isolated vertices when we have $X_i$ isolated vertices, but this breaks into two distinct events, when we count the same vertex $v_i$ twice, hence $\mathbb{E}[X_i^2]=\mathbb{E}[X]=n(1-p)^{n-1}$, and when the two vertices $v_i,v_j$ are distinct, hence $\mathbb{E}[X_iX_j]=n(1-p)^{n-1}(n-1)(1-p)^{n-2}$, because after we have $v_i$ isolated, then for $v_j$ to be isolated we have only $n-2$ other vertices that must not be adjacent to $v_j$ (because $v_i$ is already given as isolated).

\[\Rightarrow\mathbb{E}[X^2]=n(1-p)^{n-1}+n(n-1)(1-p)^{2n-3}\Rightarrow\]\[\Rightarrow{Var(X)=\mathbb{E}[X^2]-\mathbb{E}[X]^2}=\]\[=n(1-p)^{n-1}+n(n-1)(1-p)^{2n-3}-[(n(1-p)^{n-1}]^2=\]\[=n(1-p)^{n-1}+n(1-p)^{2n-3}(np-1).\]
Thus, by Chebyshev inequality,

$\mathbb{P}[X=0]\leq\mathbb{P}[|X-\mathbb{E}[X]|\geq\mathbb{E}[X]]\leq\frac{Var(X)}{\mathbb{E}[X]^2}=\frac{1}{\mathbb{E}[X]}+\frac{np-1}{n(1-p)}$. 

But $\mathbb{E}[X]\rightarrow\infty\Rightarrow\frac{1}{\mathbb{E}[X]}\rightarrow{0}\Rightarrow\mathbb{P}[X=0]\leq\frac{np-1}{n(1-p)}\leq\frac{p}{1-p}$.

But $p=\frac{c\log{n}}{n}\rightarrow{0}$ as $n\rightarrow\infty$, for any constant $c$, hence, 
\[\lim_{n\rightarrow\infty}\mathbb{P}[X=0]=0\Rightarrow\lim_{n\rightarrow\infty}\mathbb{P}[X>0]=1.\]
Thus, for any small constant $c>0$, a.a.s there exists at least one isolated vertex, that is, $G$ is not connected.
Thus, in conclusion, the probability $p=p(n)=\frac{c\log{n}}{n}$, where $c>0$ is any small constant, $p(n)$ is a threshold for $G$ to have at least one triangle, and a threshold for $G$ to have at least one isolated vertex (so not connected), and because $G(n,m)$ assumes the thresholds from $G(n,p)$, through the relation $m\approx\binom{n}{2}p$, we get that $m\approx\binom{n}{2}p(n)=\binom{n}{2}\frac{c\log{n}}{n}=\frac{cn(n-1)\log{n}}{2n}=\frac{c}{2}(n-1)\log{n}\rightarrow\infty$, as $n\rightarrow\infty$.
Thus, we get that a.a.s the graph process has at least one triangle before it is connected.
\section{}
If a graph is planar, then $G$ is $5$-degenerate. We say that $G$ is $k$-degenerate, if for every permutation $\sigma$ of the graph $n$ vertices, each vertex $\sigma(i)$ can be adjacent with no more than $k$ distinct vertices $\sigma(j)$, s.t. $\sigma(j)>\sigma(i)$. But then $G$ has no more than $kn$ in our case $5n$ possible edges. Hence, the number of choices of $m$ edges out of $5n$ possible edges if $\binom{5n}{m}$, and there are $n!$ permutations on $n$, that is, we have $n!\binom{5n}{m}$ choices for $G$. Thus, the probabiity to have $G\sim{G(n,m)}$ a $5$-degenerate graph is $p^\ast=\frac{n!\binom{5n}{m}}{\binom{N}{m}}$, where $N=\binom{n}{2}$.

But $p^\ast=\frac{n!\binom{5n}{m}}{\binom{N}{m}}=n!\frac{5n(5n-1)(5n-2)\cdots(5n-m+1)}{N(N-1)(N-2)\cdots(N-m+1)}$.

We claim that $N\geq5n$, starting from some $n_0$, and we check,

Assume that $\binom{n_0}{2}=\frac{n_0(n_0-1)}{2}\geq5n_0\Rightarrow{n_0-1\geq10\Rightarrow{n_0\geq11}}$.

Hence, for every $n\geq11$,

$\frac{5n-k}{N-k}\leq\frac{5n-(k+1)}{N-(k+1)}$, for every $0\leq{k}\leq{m-1}$ 

(because $5nN-kN-N-5nk+k^2+k\leq5nN-5nk-5n-kN+k^2+k$, for $N\geq5n$).

Thus $p^\ast=n!\frac{5n(5n-1)(5n-2)\cdots(5n-m+1)}{N(N-1)(N-2)\cdots(N-m+1)}\leq\frac{n!(5n)^m}{N^m}$.

Hence, if $m=(1+\epsilon)n$, then,

\[
p^\ast=n!(\frac{5n}{N})^m=n!(\frac{5n}{N})^{(1+\epsilon)n}=n!(2\frac{5n}{n(n-1)})^{(1+\epsilon)n}=n!(\frac{10}{n-1})^{(1+\epsilon)n}.
\]
\section{}
\subsection{}
For $n$ vertices, we have $n!$ permutations, but Hamilton cycles are indifferent to the starting point (since they are \textbf{cycles}), hence we divide the number of permutations by the number of starting points for each cycle, that is $n$. Hamilton cycles are also indifferent to direction, hence we divide this count by $2$ and we get $\frac{(n-1)!}{2}$. For each permutation $\sigma$ of the $n$ vertices, each edge in $\mu(F)$ comes from an edge between two vertices in the consecutive layers of $W$ ordered by $\sigma$, but that is $d$ choices of vertices from layers $\sigma(1),\sigma(2)$, then $d-1$ choices of the remaining vertices from layer $\sigma(k)$ together with $d$ choices of vertices from layer $\sigma(k+1)$, for every $2\leq{k}\leq{n-1}$, then $d-1$ choices of the remaining vertices from layers $\sigma(n),\sigma(1)$, but because all the choices are independent, we have $d^2(d(d-1))^{n-2}(d-1)^2=(d(d-1))^n$ choices.
\section*{4}
\subsection*{4.a}

The number of possible Hamilton circles in a multi-graph with $n$ vertices is: $(n-1)!/2$.
(There are $n!$ ways to arrange n vertices. We need to divide by $n$ because of radial symmetry. We need to divide by 2 because the direction doesn't matter.)
To get such a circle in $\mu(F)$ we need $F$ to connect between the $n$ layers of $W$. In order to connect the first two layers we need to choose from $d$ points of the first and $d$ points of the second layer. To connect the second to the third, we need to choose from $d-1$ points of the second layer and $d$ points of the third, etc. In each layer of the $n$ layers we choose two points, Therefore, there are $(d(d-1))^n$ ways to build each circle.\\
Now we need to calculate the probability that the indicator telling that $F$ having these connections equals 1.
Since $F$ is built from uniform distribution, all we need to do is to calculate the ratio between the number of matchings containing the links we need and the total number of matchings. 
The total number of matchings is: $\frac{(nd)!}{2^{nd/2}(nd/2)!}$.\\
(We need to choose all $nd$ points. we divide by $2^{nd/2}$ because there are $nd/2$ pairs. We divide by $(nd/2)!$ because the inner order of the pairs doesn't matter.)\\
The number of matchings that contain the links we need is: $\frac{(n(d-2))!}{2^{n(d-2)/2}(n(d-2)/2)!}$. \\
(Here there are only $d-2$ points to pair in each layer.)\\
Overall:

\begin{align*}
\mathbb{E}[H] 
  &= \sum_{\text{every possible Hamilton circle}}
      \mathbb{E}\!\left[ \mathbf{1}_{\{\text{the circle is in }\mu(F)\}} \right] \\[6pt]
  &= \frac{(n-1)!}{2}(d(d-1))^n
      \frac{\tfrac{(n(d-2))!}{2^{n(d-2)/2}(n(d-2)/2)!}}
           {\tfrac{(nd)!}{2^{nd/2}(nd/2)!}} \\[6pt]
  &= \frac{(n-1)!}{2}(d(d-1))^n
      \frac{(n(d-2))! \, 2^{nd/2}(nd/2)!}
           {(nd)! \, 2^{n(d-2)/2}(n(d-2)/2)!}
\end{align*}
\hfill $\square$

\subsection*{4.b}
We will use Sterling's formula:
\begin{align*}
\mathbb{E}[H] 
&\approx \frac{\sqrt{2\pi n}\,\left(\tfrac{n}{e}\right)^n}{2n}
    \cdot (d(d-1))^n \cdot \\[6pt]
&\quad \cdot
    \frac{\sqrt{2\pi n(d-2)}\left(\tfrac{n(d-2)}{e}\right)^{n(d-2)} \cdot 2^{nd/2}\cdot\sqrt{\pi nd}\cdot\left(\tfrac{nd/2}{e}\right)^{nd/2}}
         {\sqrt{\pi nd/2}\left(\tfrac{n(d-2)/2}{e}\right)^{n(d-2)/2} \cdot 2^{nd/2}\cdot2^{-n}\cdot\sqrt{2\pi nd}\cdot\left(\tfrac{nd}{e}\right)^{nd}} \\[6pt]
&= \frac{\sqrt{2\pi n}\,\left(\tfrac{n}{e}\right)^n}{2n} \cdot (d(d-1))^n \cdot \\[6pt]
&\quad \cdot 2^n \cdot e^{nd + \tfrac{n(d-2)}{2} - \tfrac{nd}{2} - n(d-2)} \cdot
   n^{-nd - \tfrac{n(d-2)}{2} + \tfrac{nd}{2} + n(d-2)} \\
&\quad \cdot (d-2)^{\tfrac{n(d-2)}{2}} \cdot d^{-nd/2}\cdot 2^{-n} \\[6pt]
&= \frac{\sqrt{2\pi n}\,\left(\tfrac{n}{e}\right)^n}{2n} \cdot (d(d-1))^n\cdot e^n \cdot n^{-n}
    \cdot d^{-\tfrac{nd}{2}} \cdot (d-2)^{\,n(-1+\tfrac{d}{2})} \\[6pt]
&= \sqrt{\frac{\pi}{2n}} \,(d-1)^n \, d^{\,n(1-\tfrac{d}{2})} \cdot (d-2)^{\,-n(1-\tfrac{d}{2})} \\[6pt]
&\approx \sqrt{\frac{\pi}{2n}} \,\Big[(d-1)\,\left(\tfrac{d-2}{d}\right)^{\tfrac{d-2}{2}}\Big]^n.
\end{align*} \hfill $\square$

\subsection*{4.c}

\subsection*{4.d}
The number of 2-factors in the multi-graph is bounded by: $\frac{(2n)!}{n!2^n}$.\\
(There are $2n$ points to arrange, 2 from each layer to get a 2-regular subgraph. We divide by $n!$ because the order of pairing doesn't matter. we divide by $2^n$ because the inner order doesn't matter. This is only a boundary because we allow to choose two points from the same layer (plus, we might have multi edges).)\\
The number of ways we can build each 2-factor is: $\binom{d}{2}^n$.\\
(We need to choose twice from each layer)\\
As in (a), we need to calculate the probability that $F$ has the needed matchings.\\
The number of possible configurations is: $\frac{(nd)!}{(\tfrac{nd}{2})!2^{\tfrac{nd}{2}}}$\\
The number of configurations which contain the needed matchings for a certain 2-factor is: $\frac{(n(d-2))!}{(\tfrac{n(d-2)}{2})!2^{\tfrac{n(d-2)}{2}}}$\\
The probability we are looking for is the ratio of the last two. Overall:\\
\begin{align*}
\mathbb{E}[T] 
  &\leq \binom{d}{2}^n\cdot \frac{(2n)!}{n!2^n} \cdot \frac{(n(d-2))!(\tfrac{nd}{2})!2^{\tfrac{nd}{2}}}{(\tfrac{n(d-2)}{2})!2^{\tfrac{n(d-2)}{2}}(nd)!}
\end{align*}
\hfill $\square$

\subsection*{4.e}
Let $T_G$ be the random variable counting the 2-factors of a n-vertices d-regular graph $G$. Notice that every multi-graph without loops and multi-edges i.e. simple made by the configuration model is a d-regular graph. Therefore, $T_G = T | \, \mu(F) \, \text{is simple}$.\\
\[
\Longrightarrow{} \mathbb{E}[T_G]=\mathbb{E}[T | \, \mu(F) \, \text{is simple}]\underset{\text{total expectation}}{\leq} \frac{\mathbb{E}[T]}{\mathbb{P}(\mu(F) \, \text{is simple})}\underset{\substack{\text{shown in } \\ \text{the lecture}}}{= }e^{\tfrac{d^2-1}{4}} O(\mathbb{E}[T])
\]
Now we can use Markov's inequality:\\
\[
\mathbb{P}(T_G>n\mathbb{E}[T])\leq\frac{\mathbb{E}[T_G]}{n\mathbb{E}[T]}=\frac{e^{\tfrac{d^2-1}{4}} O(\mathbb{E}[T])}{n\mathbb{E}[T]} \leq \frac{c}{n}\xrightarrow{n \to \infty}0
\]
Therefore, a.a.s $T_G<n\mathbb{E}[T]$.\\
\text{                                        }\hfill $\square$

\subsection*{4.f}
\begin{align*}
\frac{\mathbb{E}[H_G]}{\mathbb{E}[T_G]}
&\underset{\text{(c),(e)}}{\ge} 
\frac{\mathbb{E}[H]}{n^2 \mathbb{E}[T]} \\[2mm]
&\ge 
\frac{
\dfrac{(n-1)!}{2} (d(d-1))^n \cdot
\dfrac{(n(d-2))! \, 2^{nd/2} (nd/2)!}{(nd)! \, 2^{n(d-2)/2} (n(d-2)/2)!}
}{
n^2 \cdot \binom{d}{2}^n \cdot 
\dfrac{(2n)!}{n! 2^n} \cdot 
\dfrac{(n(d-2))! (\tfrac{nd}{2})! 2^{nd/2}}{(\tfrac{n(d-2)}{2})! 2^{n(d-2)/2} (nd)!}
} \\[2mm]
&=
\frac{
\dfrac{1}{2} (n-1)! (d(d-1))^n
}{
n^2 \cdot \dfrac{(d(d-1))^n}{2^n} \cdot \dfrac{(2n)!}{n! 2^n}
}\\[2mm]
&=\frac{1}{2}\cdot2^{2n}\cdot n^{-2}\cdot\frac{n!(n-1)!}{(2n)!
}\\[2mm]
&\approx\frac{1}{2}\cdot2^{2n}\cdot n^{-3}\cdot\frac{2\pi n(\frac{n}{e})^{2n}}{\sqrt{2\pi 2n}(\frac{2n}{e})^{2n}
}\\[2mm]
&=\frac{1}{2}\cdot2^{2n}\cdot2^{-2n}\cdot n^{-2.5}\sqrt{\pi
}\\[2mm]
&\ge\frac{1}{2} n^{-2.5}
\end{align*}

\hfill $\square$

\subsection*{4.g}
We will use the construction from Tutte's paper. Let $G'$ be the new graph(for $f(a)=2\  \ \forall a \in V$).\\
We will use the perfect matching algorithm on $G'$. The matched edges define a spanning subgraph of $G'$.
Let $a \in V$, we check how do the vertices in $X(a)\cup Y(a)$ match.\\
All the $s(a)$ vertices in $Y(a)$ have to be paired to vertices in $X(a)$, since they are connected only to vertices of $X(a)$ according to the construction. Now we need to check how many vertices of $X(a)$ are not paired to vertices from $Y(a)$:\\
\[
d(a)-s(a)=d(a)-(d(a)-f(a))=f(a)=2
\]\\
Therefore, there are exactly two vertices in $X(a)$ who are not connected to vertices of $St(a)$ (they cannot be matched together according to the construction).\\
Therefore, there are two original edges of $G$ connecting $St(a)$ to two other stars. Since this is true for every single $a \in V$, all the edges together define a 2-factor on $G$.
\hfill $\square$

\subsection*{4.h}
phase 1: as in (g), we build $G'$, and use the perfect matching algorithm. We get a 2-factor on $G$.\\
phase 2: in (f) we proved that the ratio between the number of the Hamilton circles to the number of 2-factors in a d-regular graph is at least $\frac{1}{2n^{2.5}}$ a.a.s.. Therefore if we repeat phase 1 at least $n^{3.5}$ times we have:
\[
\mathbb{P}(\{\substack{\text{not getting Hamilton} \\ \text{circle even once}}\})\leq(1-\frac{1}{2n^{2.5}})^{n^{3.5}}=((1-\frac{1/2}{n^{2.5}})^{n^{2.5}})^n\approx e^{-\frac{1}{2}n}\xrightarrow{n \to \infty}0
\]\\
Therefore a.a.s. we get a Hamilton circle in $G$.\\
Since the matching algorithm is polynomial, and we use it $n^{3.5}$ times, the total algorithm is polynomial.
\hfill $\square$
\section{}
$G=G(V,E)$.
\subsection{}
We define the following first order formula,

$\varphi(x)=\exists{y,z}$ . $y\neq{x}\land{z\neq{x}}\land{y\neq{z}}\land{x\sim{y}}\land{y\sim{z}}\land{z\sim{x}}$

Or, under the assumption that $G$ is always a simple graph,

$\varphi(x)=\exists{y,z}$ . $x\sim{y}\land{y\sim{z}}\land{z\sim{x}}$

Denote $U:=\{u\in{V}\,|\,{G\vDash\varphi(u)}\}$, then for every $x$, either $x\in{U}$ or $x\notin{U}$.
\subsection{}
\subsection{}
The empty set can be defined by the following first order formula,

$\varphi_\phi(x)=x\neq{x}$, which is a contradiction, hence $U_\phi:=\{u\in{V}\,|\,{G\vDash\varphi_\phi(u)}\}=\phi$.

The set of all graph vertices can be defined by the following first order formula,

$\varphi(x)=x=x$, which is a tautology, hence $U_V:=\{u\in{V}\,|\,{G\vDash\varphi(u)}\}=V$.

Both formulas are either true or false for all $v\in{V}$, therefore the sets that satisfy them are called the trivial definable sets.

Comment: there are specific symbols in first order languages, to specify tautology and contradiction, but I prefer the above sentences.
\subsection{}
The required statement would be,
\noindent
\[\varphi'=\forall{0\leq{p}\leq{1}},\epsilon>0\,.\,\exists{N}\,.\,\forall{n>N},\varphi\,.\,\]\[.\,\varphi=\varphi_\phi\lor\varphi=\varphi_V\lor\mathbb{P}[G(n,p)\vDash\varphi]<\epsilon\]

Another version of $\varphi'$,
\noindent
\[\varphi'=\forall{0\leq{p}\leq{1}},\epsilon>0\,.\,\exists{N}\,.\,\forall{n>N},\varphi\,.\,\]\[.\,\neg(\exists{v,u\in{V}}\,.\,\varphi\neq\varphi_\phi\land\varphi\neq\varphi_V\land\mathbb{P}[G(n,p)\vDash\varphi(u)]\geq\epsilon\land\mathbb{P}[G(n,p)\neg\vDash\varphi(v)]\geq\epsilon)\]
\subsection{}
This should be immediate from Fagin's 0-1 law, but I shall describe my idea of a proof.

For every $x\in{V}$, a sentence $\varphi(x)$ is a finite set of statements which can contain a finite subset of the graph vertices $x_1,x_2\dots,x_k\in{V}$, for some $k<\infty$, the quantifiers $\exists,\forall$, the relations $=,\sim$, and the negation unary logical operator $\neg$. These statements are related to each other by the logical operators $\land,\lor$.

The idea of the proof is by induction on the total number of elements in the sentence.

We start from one statement with one quantifier, one vertex and one relation.

$\varphi(x)=\exists{y\in{V}}\,.\,y={x}$ would be a tautology, so we take 

$\varphi(x)=\exists{y\in{V}}\,.\,y\neq{x}\land{y={x}}$ (we actually added more elements implicitly, but that does not matter).

Fix $n>0$, $\mathbb{P}[G\vDash\varphi(x)]$ is the probability that there exists an edge between $x$ and some other vertex in the graph, which is the completion of the event that $x$ does not have an edge with any other graph vertex, that is \[\mathbb{P}[G\sim{G(n,p)}\neg\vDash\varphi(x)]=\prod_{x\neq{y}}\mathbb{P}[x\nsim{y}]=\prod_{y\neq{x}}1-\mathbb{P}[x\sim{y}]=\]\[=\prod_{i=1}^{n-1}1-p=(1-p)^{n-1}=\frac{(1-p)^n}{1-p}\]
but \[\lim_{n\rightarrow\infty}\frac{(1-p)^n}{1-p}=0\]
hence for every $x\in{V}$, \[\lim_{n\rightarrow\infty}\mathbb{P}[G(n,p)\vDash\varphi(x)]=1-\lim_{n\rightarrow\infty}\mathbb{P}[G(n,p)\neg\vDash\varphi(x)]=1\Rightarrow{V_\varphi={V}}.\]
This assumes that $p$ is constant for every $n>0$, otherwise there might not be a convergence (to zero).

Same way, we can prove that if $\varphi(x)=\forall{y\neq{x}}\,.\,x\sim{y}$, then \[\lim_{n\rightarrow\infty}\mathbb{P}[G(n,p)\vDash\varphi(x)]=\lim_{n\rightarrow\infty}p^{n-1}=\lim_{n\rightarrow\infty}\frac{p^n}{p}=0\Rightarrow{V_\varphi=\phi}.\]
Thus we assume that for any sentence $\varphi$ of $m$ elements, either

\[\lim_{n\rightarrow\infty}\mathbb{P}[G\sim{G(n,p)\vDash\varphi}]=0\Rightarrow\lim_{n\rightarrow\infty}\mathbb{P}[G\sim{G(n,p)\vDash\neg\varphi}]=1\]

or,

\[\lim_{n\rightarrow\infty}\mathbb{P}[G\sim{G(n,p)\vDash\varphi}]=1\Rightarrow\lim_{n\rightarrow\infty}\mathbb{P}[G\sim{G(n,p)\vDash\neg\varphi}]=0\]

We need to prove for $m+1$ elements. Suppose that the additional element is another statement $\varphi'$, the the relation between them is either $\varphi\land\varphi'$ or $\varphi\lor\varphi'$, but by the assumption, $\varphi,\varphi'$ have limit probabilities, thus \[\lim_{n\rightarrow\infty
}\mathbb{P}[G\sim{G(n,p)}\vDash(\varphi\land\varphi')]=\lim_{n\rightarrow\infty
}\mathbb{P}[G\sim{G(n,p)}\vDash\varphi]\cdot\lim_{n\rightarrow\infty
}\mathbb{P}[G\sim{G(n,p)}\vDash\varphi']\]

and,

\[\lim_{n\rightarrow\infty
}\mathbb{P}[G\sim{G(n,p)}\vDash(\varphi\lor\varphi')]=\lim_{n\rightarrow\infty
}\mathbb{P}[G\sim{G(n,p)}\vDash\neg(\neg\varphi\land\neg\varphi')]=\]\[=1-\lim_{n\rightarrow\infty
}\mathbb{P}[G\sim{G(n,p)}\vDash\neg\varphi]\cdot\lim_{n\rightarrow\infty
}\mathbb{P}[G\sim{G(n,p)}\neg\vDash\varphi']\]
\end{document}
